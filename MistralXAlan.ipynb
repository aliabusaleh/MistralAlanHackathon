{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation for Medical Knowledge Evaluation Solution\n",
    "## Introduction\n",
    "\n",
    "Welcome to our journey in enhancing medical knowledge evaluation! In this project, we are tackling a unique challenge presented by the French Medical Practice exam. Our objective is to improve a model's ability to answer 103 multiple-choice questions, each with options ranging from A to E. \n",
    "\n",
    "What makes this task especially intriguing is that many questions may have multiple correct answers, leading to a total of 31 possible answer combinations. This complexity pushes us to think creatively about how to equip our model with the necessary medical knowledge.\n",
    "\n",
    "While we don’t have a training dataset with the correct answers, because where’s the fun in that?—we’re excited to explore open-source resources and innovative techniques to enrich our model's understanding of medical concepts. \n",
    "\n",
    "Join us as we dive into the details of our approach, aiming to not only meet but exceed the benchmarks set by this competition. Let’s see how far we can push the boundaries of medical knowledge assessment!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r req.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables you need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('API_KEY')\n",
    "\n",
    "questions_file = './data/questions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mistralai import Mistral\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(questions_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"thedevastator/comprehensive-medical-q-a-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_rag = pd.read_csv(path+'\\\\'+\"train.csv\")\n",
    "\n",
    "# Function to combine the columns into a structured text representation\n",
    "def combine_qtype_question_answer(row):\n",
    "    return (\n",
    "        f\"Question Type: {row['qtype']}\\n\"\n",
    "        f\"Question: {row['Question']}\\n\"\n",
    "        f\"Answer: {row['Answer']}\\n\"\n",
    "    )\n",
    "\n",
    "# Apply the function to each row in the dataframe to create a combined text column\n",
    "df_rag['combined_text'] = df_rag.apply(combine_qtype_question_answer, axis=1)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # Choose the appropriate encoding model\n",
    "\n",
    "# Function to split text into chunks of max_token_size\n",
    "def split_into_chunks(text, max_token_size=2048):\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_token_size):\n",
    "        chunk = encoding.decode(tokens[i:i + max_token_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Apply the function to each combined text in the dataframe\n",
    "chunks_list = []\n",
    "for text in df_rag['combined_text']:\n",
    "    chunks = split_into_chunks(text, max_token_size=512)\n",
    "    chunks_list.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "          model=\"mistral-embed\",\n",
    "          inputs=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "# text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "text_embeddings = []\n",
    "for chunk in tqdm(chunks_list):\n",
    "    text_embeddings.append(get_text_embedding(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding arrays\n",
    "emb_arr = np.load('embdeddigs_g')  # Make sure this is a valid file path\n",
    "# emb_arr2 = np.load('embdeddigs_mcqa_red')  # Ensure this is a .npy file\n",
    "\n",
    "# Ensure that the embedding arrays have the same number of dimensions\n",
    "# if emb_arr.shape[1] != emb_arr2.shape[1]:\n",
    "#     raise ValueError(\"The dimensions of emb_arr and emb_arr2 do not match.\")\n",
    "\n",
    "# Create the FAISS index\n",
    "d = emb_arr.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add the embeddings to the index\n",
    "index.add(emb_arr)\n",
    "# index.add(emb_arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/questions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = lambda question: (\n",
    "    \"Translate the following question into English. If the question is already in English, repeat it as is.\\n\\n\"\n",
    "    f\"Question: {question}\\n\"\n",
    "    \"Translation:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "initial_answer_prompt = lambda question, possible_answer_a, possible_answer_b, possible_answer_c, possible_answer_d, possible_answer_e, context: (\n",
    "    \"Vous êtes un assistant très compétent formé pour résoudre des questions à choix multiples dans divers domaines, \"\n",
    "    \"y compris la science, les mathématiques, l'histoire et plus encore. Votre objectif est de fournir la réponse la plus précise en fonction des options présentées.\\n\\n\"\n",
    "\n",
    "\n",
    "    \"Tâche : Identifier la ou les réponses correctes pour la question à choix multiples donnée. La question peut avoir une ou plusieurs réponses correctes.\\n\"\n",
    "    \"Fournir la ou les réponses en utilisant uniquement les lettres correspondant aux options correctes.\\n\\n\"\n",
    "\n",
    "    \"# Instructions :\\n\"\n",
    "    \"1. Analysez la question pour comprendre son contexte et les informations qu'elle requiert.\\n\"\n",
    "    \"2. Évaluez chacune des options données (A, B, C, D, E) pour déterminer quelle(s) option(s) répond(ent) le mieux à la question.\\n\"\n",
    "    \"3. Si plusieurs options sont correctes, listez-les par ordre alphabétique, séparées par des virgules et sans espaces.\\n\"\n",
    "    \"4. Ne fournissez que la réponse telle que spécifiée, sans texte ou explications supplémentaires.\\n\\n\"\n",
    "\n",
    "    \"# Contraintes :\\n\"\n",
    "    \"- Si toutes les réponses sont incorrectes, retournez 'Aucune'.\\n\"\n",
    "    \"- Si la question précise 'Sélectionnez toutes les réponses applicables', plusieurs réponses peuvent être possibles.\\n\"\n",
    "    \"- Si les options incluent des informations contradictoires, choisissez l'option la plus précise ou pertinente en fonction du contexte de la question.\\n\\n\"\n",
    "    \n",
    "    \"# Examples:\\n\"\n",
    "    \"- Question: 'Which of the following are prime numbers?'\\n\"\n",
    "    \"  A: 4\\n\"\n",
    "    \"  B: 5\\n\"\n",
    "    \"  C: 7\\n\"\n",
    "    \"  D: 9\\n\"\n",
    "    \"  E: 12\\n\"\n",
    "    \"  Output: 'B,C'\\n\\n\"\n",
    "    \"- Question: 'Which animals are mammals?'\\n\"\n",
    "    \"  A: Elephant\\n\"\n",
    "    \"  B: Crocodile\\n\"\n",
    "    \"  C: Kangaroo\\n\"\n",
    "    \"  D: Snake\\n\"\n",
    "    \"  E: Dolphin\\n\"\n",
    "    \"  Output: 'A,C,E'\\n\\n\"\n",
    "    \n",
    "    \"If additional context is needed, here it is:\\n\"\n",
    "    \"--------------------\\n\"\n",
    "    f\"{context}\\n\"\n",
    "    \"--------------------\\n\\n\"\n",
    "    \"# Question:\\n\"\n",
    "    f\"{question}\\n\"\n",
    "    \"Options:\\n\"\n",
    "    f\"A: {possible_answer_a}\\n\"\n",
    "    f\"B: {possible_answer_b}\\n\"\n",
    "    f\"C: {possible_answer_c}\\n\"\n",
    "    f\"D: {possible_answer_d}\\n\"\n",
    "    f\"E: {possible_answer_e}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confirmation_prompt = lambda question, possible_answer_a, possible_answer_b, possible_answer_c, possible_answer_d, possible_answer_e, initial_answer: (\n",
    "    \"You have provided an initial answer to the multiple-choice question. Please reassess the question and options carefully.\\n\\n\"\n",
    "    \"If you believe your initial answer was correct, confirm it and explain why. If you think it was incorrect, provide the correct answer with an explanation.\\n\\n\"\n",
    "    f\"# Question:\\n{question}\\n\"\n",
    "    \"Options [Translate To English if there's not English BEFORE answer]:\\n\"\n",
    "    f\"A: {possible_answer_a}\\n\"\n",
    "    f\"B: {possible_answer_b}\\n\"\n",
    "    f\"C: {possible_answer_c}\\n\"\n",
    "    f\"D: {possible_answer_d}\\n\"\n",
    "    f\"E: {possible_answer_e}\\n\\n\"\n",
    "    f\"Previously selected answer: '{initial_answer}'\\n\\n\"\n",
    "    \"Reassess the options and provide a new answer if needed, along with a brief explanation.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_formatting_prompt = lambda question, context, final_answer: (\n",
    "    \"Inital Question was:\\n\"\n",
    "    f\"{question}\\n\\n\"\n",
    "    \"After careful evaluation, the final answer is:\\n\"\n",
    "    f\"{final_answer}\\n\\n\"\n",
    "    \"Given the context this:\\n\"\n",
    "    f\"{context}\\n\\n\"\n",
    "    \"#Task\"\n",
    "    \"Please confirm the final answer format as follows:\\n\"\n",
    "    \"- If one option is correct, format as: 'A'\\n\"\n",
    "    \"- If multiple options are correct, format as: 'A,B'\\n\"\n",
    "    \"- If no correct options, return: 'None'\\n\"\n",
    "    \"Final formatted answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder for answers\n",
    "answers = []\n",
    "\n",
    "for row_idx, row in df.iterrows():\n",
    "    question_embeddings = np.array([get_text_embedding(row[\"question\"])])\n",
    "    D, I = index.search(question_embeddings, k=4)  # distance, index\n",
    "\n",
    "    # Check if I has valid entries and retrieve chunks\n",
    "    if I.tolist() and len(I[0]) > 0:\n",
    "        # Retrieve the chunks based on available indices in I\n",
    "        retrieved_chunk = [chunks[i] for i in I[0] if i < len(chunks)]\n",
    "        retrieved_chunk = retrieved_chunk[0] if retrieved_chunk else 'No additional context available.'\n",
    "    else:\n",
    "        retrieved_chunk = 'No additional context available.'\n",
    "    # Step 1: Translate question to English if necessary\n",
    "    translation_request = translation_prompt(row[\"question\"])\n",
    "    translation_response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": translation_request}],\n",
    "        temperature=0.\n",
    "    )\n",
    "    translated_question = translation_response.choices[0].message.content.strip()\n",
    "\n",
    "    # Step 2: Initial Answer Selection\n",
    "    context = retrieved_chunk  #  use FAISS retrieval if available\n",
    "    initial_request = initial_answer_prompt(\n",
    "        translated_question,\n",
    "        row[\"answer_A\"],\n",
    "        row[\"answer_B\"],\n",
    "        row[\"answer_C\"],\n",
    "        row[\"answer_D\"],\n",
    "        row.get(\"answer_E\"),\n",
    "        context\n",
    "    )\n",
    "    initial_response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": initial_request}],\n",
    "        temperature=0.\n",
    "    )\n",
    "    initial_answer = initial_response.choices[0].message.content.strip()\n",
    "\n",
    "    # Step 3: Confirmation with Reasoning\n",
    "    confirmation_request = confirmation_prompt(\n",
    "        translated_question,\n",
    "        row[\"answer_A\"],\n",
    "        row[\"answer_B\"],\n",
    "        row[\"answer_C\"],\n",
    "        row[\"answer_D\"],\n",
    "        row.get(\"answer_E\"),\n",
    "        initial_answer\n",
    "    )\n",
    "    confirmation_response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": confirmation_request}],\n",
    "        temperature=0.\n",
    "    )\n",
    "    confirmed_answer = confirmation_response.choices[0].message.content.strip()\n",
    "\n",
    "    # Step 4: Final Result Formatting\n",
    "    result_request = result_formatting_prompt(\n",
    "        translated_question,\n",
    "        retrieved_chunk,\n",
    "        initial_answer # dump way of doing it.\n",
    "    )\n",
    "    result_response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": result_request}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    final_answer = result_response.choices[0].message.content.strip()\n",
    "\n",
    "    # Store the final formatted answer\n",
    "    answers.append(final_answer)\n",
    "    print(f\"Final Answer for Question {row_idx}: {final_answer}\")\n",
    "# output format is a 2-columns dataframe with exactly 103 rows\n",
    "output_df = pd.DataFrame(answers, columns=[\"Answer\"])\n",
    "output_df.index.name = \"id\"\n",
    "\n",
    "output_df.to_csv(\"output_multistage.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_json(df):\n",
    "    questions_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        question_entry = {\n",
    "            \"qtype\": row['qtype'],\n",
    "            \"question\": row['Question'],\n",
    "            \"answer\": row['Answer']\n",
    "        }\n",
    "        questions_list.append(question_entry)\n",
    "        print(question_entry)\n",
    "    \n",
    "    result = {\"questions\": questions_list}\n",
    "    return json.dumps(result, indent=4)\n",
    "\n",
    "df_emb = pd.read_csv(\"C:\\\\Users\\\\aligh\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\comprehensive-medical-q-a-dataset\\\\versions\\\\2\\\\train.csv\")\n",
    "\n",
    "convert_to_json(df_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "def extract_qa_pairs_to_jsonl(folder_path, output_jsonl, limit=5000):\n",
    "    qa_list = []\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through the folder and sub-folders\n",
    "    for root_dir, sub_dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(root_dir, file)\n",
    "                try:\n",
    "                    # Parse the XML file\n",
    "                    tree = ET.parse(file_path)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    # Find all QAPair elements\n",
    "                    qa_pairs = root.findall('.//QAPair')\n",
    "                    for qa in qa_pairs:\n",
    "                        if count >= limit:\n",
    "                            break\n",
    "                        # Extract Question\n",
    "                        question = qa.find('Question').text.strip()\n",
    "                        \n",
    "                        # Extract Answer if present\n",
    "                        answer = qa.find('Answer')\n",
    "                        if answer is not None and answer.text and answer.text.strip():\n",
    "                            answer_text = answer.text.strip()\n",
    "                            # Add the question and answer as a dictionary to the list\n",
    "                            qa_list.append({\"Question\": question, \"Answer\": answer_text})\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    if count >= limit:\n",
    "                        break\n",
    "\n",
    "                except ET.ParseError as e:\n",
    "                    print(f\"Error parsing file {file_path}: {e}\")\n",
    "    \n",
    "    # Save the collected Q&A pairs to a JSONL file\n",
    "    with open(output_jsonl, 'w', encoding='utf-8') as jsonlfile:\n",
    "        for qa in qa_list:\n",
    "            jsonlfile.write(json.dumps(qa, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "    print(f\"Saved {count} Q&A pairs to {output_jsonl}\")\n",
    "\n",
    "\n",
    "# Specify the path to the folder containing the XML files and the output CSV file name\n",
    "folder_path = 'C:\\\\Users\\\\aligh\\\\Downloads\\\\MedQuAD-master\\\\MedQuAD-master\\\\'\n",
    "output_csv = 'qa_pairs.jsonl'\n",
    "extract_qa_pairs_to_jsonl(folder_path, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "def extract_qa_pairs_to_mistral_jsonl(folder_path, output_jsonl):\n",
    "    conversations = []\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through the folder and sub-folders\n",
    "    for root_dir, sub_dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(root_dir, file)\n",
    "                try:\n",
    "                    # Parse the XML file\n",
    "                    tree = ET.parse(file_path)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    # Find all QAPair elements\n",
    "                    qa_pairs = root.findall('.//QAPair')\n",
    "                    for qa in qa_pairs:\n",
    "                        # Extract Question\n",
    "                        question = qa.find('Question').text.strip()\n",
    "                        \n",
    "                        # Extract Answer if present\n",
    "                        answer = qa.find('Answer')\n",
    "                        if answer is not None and answer.text and answer.text.strip():\n",
    "                            answer_text = answer.text.strip()\n",
    "                            \n",
    "                            # Structure the conversation in Mistral format\n",
    "                            conversation = {\n",
    "                                \"messages\": [\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": question\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"role\": \"assistant\",\n",
    "                                        \"content\": answer_text\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                            \n",
    "                            conversations.append(conversation)\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "\n",
    "                except ET.ParseError as e:\n",
    "                    print(f\"Error parsing file {file_path}: {e}\")\n",
    "    \n",
    "    # Save the collected conversations to a JSONL file in Mistral format\n",
    "    with open(output_jsonl, 'w', encoding='utf-8') as jsonlfile:\n",
    "        for conversation in conversations:\n",
    "            jsonlfile.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "    print(f\"Saved {count} conversations to {output_jsonl}\")\n",
    "\n",
    "\n",
    "# Specify the path to the folder containing the XML files and the output CSV file name\n",
    "folder_path = 'C:\\\\Users\\\\aligh\\\\Downloads\\\\MedQuAD-master\\\\MedQuAD-master\\\\'\n",
    "output_jsonl = 'qa_pairs_mistral.jsonl'\n",
    "extract_qa_pairs_to_mistral_jsonl(folder_path, output_jsonl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import os\n",
    "\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "training_data = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": \"qa_pairs_mistral.jsonl\",\n",
    "        \"content\": open(\"qa_pairs_mistral.jsonl\", \"rb\"),\n",
    "    }\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fine-tuning job\n",
    "created_jobs = client.fine_tuning.jobs.create(\n",
    "    model=\"mistral-large-latest\", \n",
    "    training_files=[{\"file_id\": training_data.id, \"weight\": 1}],\n",
    "    hyperparameters={\n",
    "        \"training_steps\": 10,\n",
    "        \"learning_rate\":0.0001\n",
    "    },\n",
    "    auto_start=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start a fine-tuning job\n",
    "client.fine_tuning.jobs.start(job_id = created_jobs.id)\n",
    "\n",
    "created_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cancel a jobs\n",
    "# canceled_jobs = client.fine_tuning.jobs.cancel(job_id = created_jobs.id)\n",
    "# print(canceled_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a jobs\n",
    "retrieved_jobs = client.fine_tuning.jobs.get(job_id = created_jobs.id)\n",
    "retrieved_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MistralAlan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
